{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 不同模型效果比较\n",
    "本节将通过MNIST数据集来比较第4章中提到的不同优化方法对神经网络模型正确率的影响，使用神经网络模型在MNIST测试数据集上的正确率作为评价标准。（不同模型的运行代码文件见本文件同目录下文件夹内。）\n",
    "\n",
    "在第4章中提到了设计神经网络时的5种优化方法。**在神经网络结构的设计上，需要使用激活函数、多层隐藏层；在神经网络优化时，可以使用指数衰减学习率、正则化损失函数、滑动平均模型。**下图给出了相同参数下（唯一不同是不使用激活函数时，学习率为0.05），使用不同优化方法的结果对比：\n",
    "<p align='center'>\n",
    "    <img src=../images/图5.3.JPG>\n",
    "</p>\n",
    "\n",
    "上图结果包含了使用所有优化方法训练得到的模型和不用其中某一项优化方法训练得到的模型。通过这种方式，可以有效验证每一项优化方法的效果。另外上图中迭代轮数为30000，且由于神经网络训练过程存在随机性，本节中列出所有结果都是10次运行的平均值。\n",
    "\n",
    " - 从图5.3中可以很明显地看出，调整神经网络的结构对最终的正确率有非常大的影响。没有隐藏层或者没有激活函数时，模型的正确率只有大约92.6%，这个数字要远远小于使用了隐藏层和激活函数时可以达到的大约98.4%的正确率。**这说明神经网络的结构对最终模型的效果有本质性的影响**。第6章将会介绍一种更加特殊的神经网络结构一一**卷积神经网络**，它可以更加有效地处理图像信息，可以进一步将正确率提高到大约99.5%。\n",
    "\n",
    " - 从图5.3中还可发现使用滑动平均模型、指数衰减的学习率和使用正则化带来的正确率的提升并不是特别明显。其中使用了所有优化算法的模型和不使用滑动平均的模型以及不使用指数衰减的学习率的模型都可以达到大约98.4%的正确率。这是因为滑动平均模型和指数衰减的学习率在一定程度上都是限制神经网络中参数更新的速度，**然而在MNIST数据上，因为模型收敛的速度很快，所以这两种优化对最终模型的影响不大。**\n",
    "<p align='center'>\n",
    "    <img src=../images/图5.4.JPG>\n",
    "</p>\n",
    "\n",
    "图5.4显示了不同迭代轮数时，使用了所有优化方法的模型的正确率与平均绝对梯度(指所有参数梯度平均值的平均数)的变化趋势，可以看到**前4000轮对模型的改变时最大的，4000轮之后因为梯度本身比较小，所有参数的改变也就比较缓慢了，于是滑动平均模型或者指数衰减模的学习率的作用也就没有那么突出了。**\n",
    "<p align='center'>\n",
    "    <img src=../images/图5.5.JPG>\n",
    "</p>\n",
    "\n",
    "图5.5显示了不同迭代轮数时，正确率与衰减之后的学习率的变化趋势。可以看到，学习率曲线呈现出阶梯衰减。在前4000轮时，衰减之后的学习率和最初的学习率差距并不大。**那么，这是否能说明这些优化方法作用不大呢？答案是否定的。当问题更加复杂时，迭代不会这么快接近收敛，这时滑动平均模型和指数衰减的学习率可以发挥更大的作用**。比如在CIFAR-10图像分类数据集上，使用滑动平均模型可以将错误率降低11%，而使用指数衰减的学习率可以将错误率降低7%。\n",
    "<p align='center'>\n",
    "    <img src=../images/图5.6.JPG>\n",
    "    <img src=../images/图5.7.JPG>\n",
    "</p>\n",
    "\n",
    "图5.6和图5.7显示了正则化给模型优化过程带来的影响。**相比滑动平均模型和指数衰减模型，使用正则化的损失函数给模型效果带来的提升要相对显著**。使用了正则化的模型可以大约降低6%的错误率（从1.69%到1.59%）。\n",
    "\n",
    "从图5.6可以看出，只优化交叉熵的模型在训练数据上的交叉熵损失（灰色虚线）要比优化总损失的模型更小（黑色虚线），然而在测试数据上，优化总损失的模型（黑色实线）却要好于只优化交叉熵的模型（灰色实线），这个原因就是第4章中介绍的**过拟合**问题。只优化交叉熵的模型可以更好地拟合训练数据（交叉熵损失更小），但是却不能很好地挖掘数据中潜在的规律来判断未知的测试数据，所以在测试数据上的正确率低。\n",
    "\n",
    "图5.7显示了不同模型的损失函数的变化趋势。图5.7的左侧显示了只优化交叉熵的模型损失函数的变化规律。可以看到随着迭代的进行，正则化损失是在不断加大的。因为MNIST问题相对比较简单，迭代后期的梯度很小（参考图5.4），所以正则化损失的增长也不快。如果问题更加复杂，选代后期的梯度更大，就会发现总损失（交叉熵损失加上正则化损失）会呈现出一个U字型。在图5.7的右侧显示了优化总损失的模型损失函数的变化规律。从图5.7中可以看出，这个模型的正则化损失部分也可以随着迭代的进行越来越小，从而使得整体的损失呈现一个逐步递减的趋势。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
